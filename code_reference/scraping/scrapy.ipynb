{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shell\n",
    "test webpage data extraction in terminal\n",
    "### in terminal:\n",
    "start shell: `scrapy shell`  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### in scrapy shell:\n",
    "- direct to page: `fetch(\"http://www.alice-in-wonderland.net/resources/chapters-script/alices-adventures-in-wonderland/chapter-1/\")`  \n",
    "- view downloaded data: `view(response)`\n",
    "- scrape text data: `response.xpath('//h1/text()').extract()[0]`\n",
    "    - output: `'Chapter 1: Down the Rabbit-Hole'`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set-up Project\n",
    "data collection directories and files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## in terminal:\n",
    "- create project: `scrapy startproject *project_name*`\n",
    "    - creates `project_name` directory\n",
    "- `project_name` folder structure:\n",
    "    - `project_name` folder\n",
    "        - `__pycache__` folder\n",
    "        - `spiders` folder\n",
    "            - `__pycache__` folder\n",
    "            - `__init__.py` file\n",
    "        - `__init__.py` file\n",
    "        - `items.py` file\n",
    "        - `middlewares.py` file\n",
    "        - `pipelines.py` file\n",
    "        - `settings.py` file\n",
    "    - `scrapy.cfg` file\n",
    "- create `*scraper*.py` file in `project_name/project_name/spiders` folder:\n",
    "    - `touch project_name/project_name/spiders/scraper.py`\n",
    "    - create spider scraper in this file\n",
    "- run spider (in project top level directory): \n",
    "    - `cd project_name`\n",
    "    - `scrapy crawl *spider_name*`\n",
    "- output file located in project top level directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## example - scrapy_alice\n",
    "- `scrapy startproject scrapy_alice`\n",
    "- `touch scrapy_alice/scrapy_alice/spiders/scraper.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scraper.py script\n",
    "in spiders directory:\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "\n",
    "# create spider class for scraping\n",
    "class AliceSpider(scrapy.Spider):\n",
    "\n",
    "    # name to call spider in terminal\n",
    "    name = 'alice_spider'\n",
    "\n",
    "    # set spider settings\n",
    "    custom_settings = {\n",
    "        \"DOWNLOAD_DELAY\": 3, # wait time between downloading pages\n",
    "        \"CONCURENT_REQUESTS_PER_DOMAIN\": 3, # max concurrent requests\n",
    "        \"FEED_FORMAT\": 'csv', # output file type\n",
    "        \"FEED_URI\": 'alice_data.csv' # output file name\n",
    "    }\n",
    "\n",
    "    # initial webpages\n",
    "    start_urls = [\n",
    "        'http://www.alice-in-wonderland.net/resources/chapters-script/alices-adventures-in-wonderland/chapter-1/'\n",
    "    ]\n",
    "    \n",
    "    # this initial function must be called \"parse\"\n",
    "    # function to list webpages\n",
    "    def parse(self, response):\n",
    "        # iterate through webpage hyperlink elements\n",
    "        for link in response.xpath('//ul[@class=\"sub-menu-ul\"]/ul/li/a/@href').extract():\n",
    "            # load each page and run \"parse_webpage\" function\n",
    "            yield scrapy.Request(\n",
    "                url=link, # url for request\n",
    "                callback=self.parse_webpage, # function to be called\n",
    "                meta={'url': link} # metadata\n",
    "            )\n",
    "\n",
    "    # function to retrieve data from each webpage\n",
    "    def parse_webpage(self, response):\n",
    "        # load webpage for scraping\n",
    "        url = response.request.meta['url']\n",
    "        # scrape elements\n",
    "        title = response.xpath('//h1/text()').extract()[0]\n",
    "        first_paragraph = response.xpath('//main/article/p/text()').extract()[0]\n",
    "\n",
    "        # yield scraped data (to file)\n",
    "        yield {\n",
    "            'title': title,\n",
    "            'paragraph': first_paragraph\n",
    "        }\n",
    "        \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run spider\n",
    "- `cd scrapy_alice`\n",
    "- `scrapy crawl alice_spider`\n",
    "- alice_data.csv file created in scrapy_alice folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
